{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Errors\n",
    "\n",
    "Here are two datasets that represent two of the examples you have seen in this lesson.  \n",
    "\n",
    "One dataset is based on the parachute example, and the second is based on the judicial example.  Neither of these datasets is based on real people.\n",
    "\n",
    "Use the exercises below to assist in answering the quiz questions at the bottom of this page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "jud_data = pd.read_csv('judicial_dataset_predictions.csv')\n",
    "par_data = pd.read_csv('parachute_dataset.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>defendant_id</th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22574</td>\n",
       "      <td>innocent</td>\n",
       "      <td>innocent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35637</td>\n",
       "      <td>innocent</td>\n",
       "      <td>innocent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39919</td>\n",
       "      <td>innocent</td>\n",
       "      <td>innocent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29610</td>\n",
       "      <td>guilty</td>\n",
       "      <td>guilty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38273</td>\n",
       "      <td>innocent</td>\n",
       "      <td>innocent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   defendant_id    actual predicted\n",
       "0         22574  innocent  innocent\n",
       "1         35637  innocent  innocent\n",
       "2         39919  innocent  innocent\n",
       "3         29610    guilty    guilty\n",
       "4         38273  innocent  innocent"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jud_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['opens', 'fails'], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par_data.actual.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` Above, you can see the actual and predicted columns for each of the datasets.  Using the **jud_data**, find the proportion of errors for the dataset, and furthermore, the percentage of errors of each type.  Use the results to answer the questions in quiz 1 below.  \n",
    "\n",
    "**Hint for quiz:** an error is any time the prediction doesn't match an actual value.  Additionally, there are Type I and Type II errors to think about.  We also know we can minimize one type of error by maximizing the other type of error.  If we predict all individuals as innocent, how many of the guilty are incorrectly labeled?  Similarly, if we predict all individuals as guilty, how many of the innocent are incorrectly labeled?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Total percentage errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>defendant_id</th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>29430</td>\n",
       "      <td>guilty</td>\n",
       "      <td>innocent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>29092</td>\n",
       "      <td>guilty</td>\n",
       "      <td>innocent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>29012</td>\n",
       "      <td>guilty</td>\n",
       "      <td>innocent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>26079</td>\n",
       "      <td>guilty</td>\n",
       "      <td>innocent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>25523</td>\n",
       "      <td>guilty</td>\n",
       "      <td>innocent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    defendant_id  actual predicted\n",
       "42         29430  guilty  innocent\n",
       "44         29092  guilty  innocent\n",
       "49         29012  guilty  innocent\n",
       "56         26079  guilty  innocent\n",
       "71         25523  guilty  innocent"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jud_data_allErrrors = jud_data[jud_data.actual != jud_data.predicted]\n",
    "jud_data_allErrrors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total % of Errors: 0.04215\n"
     ]
    }
   ],
   "source": [
    "total_errors = jud_data_allErrrors.shape[0]/jud_data.shape[0] \n",
    "\n",
    "print('Total % of Errors: {:.4}'.format(total_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3709"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jud_pred_guilty = jud_data.query('predicted == \"guilty\"')\n",
    "jud_pred_guilty.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3574"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jud_pred_innocent = jud_data.query('predicted == \"innocent\"') \n",
    "jud_pred_innocent.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Percentage of judicial Type1 errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jud_type1Error_percent: 0.001510366607167376'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jud_type1Error_percent = jud_pred_guilty.query('actual != \"guilty\"').shape[0]/ jud_data.shape[0]\n",
    "f'jud_type1Error_percent: {jud_type1Error_percent}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Percentage of judicial Type2 errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jud_type2Error_percent: 0.04064259233832212'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jud_type2Error_percent = jud_pred_innocent.query('actual != \"innocent\"').shape[0]/ jud_data.shape[0]\n",
    "f'jud_type2Error_percent: {jud_type2Error_percent}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Proportion of errors for error1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jud_type1_proportion: 0.45159961554304545'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we assume everybody is guilty so we judge as guilty - Ho: == True\n",
    "# hence the errors are where actual == innocent\n",
    "\n",
    "jud_type1_proportion =  jud_data.query('actual == \"innocent\"').shape[0]/ jud_data.shape[0]\n",
    "f'jud_type1_proportion: {jud_type1_proportion}'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Proportion of errors for error2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#If everyone has prediction of guilty, then no one is predicted inncoent\n",
    "#Therefore, there would be no type 2 errors in this case\n",
    "\n",
    "# Type II errs = pred innocent, but actual = guilty\n",
    "0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Using the **par_data**, find the proportion of errors for the dataset, and furthermore, the percentage of errors of each type.  Use the results to answer the questions in quiz 2 below.\n",
    "\n",
    "These should be very similar operations to those you performed in the previous question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### par_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Total percentage of errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total percentage errors in parachute 0.039972551037913875'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par_data_err = par_data[par_data['actual'] != par_data['predicted']]\n",
    "par_total_errors = par_data_err.shape[0]/par_data.shape[0]\n",
    "par_total_errors\n",
    "f'Total percentage errors in parachute {par_total_errors}' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Percentaage of type1 errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['opens', 'fails'], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par_data.predicted.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' par_type1Error: 0.00017155601303825698'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here actaul = fails, predicted = opens\n",
    "par_type1Error = par_data[par_data['actual'] == 'fails'].query('predicted == \"opens\"').shape[0]/par_data.shape[0]\n",
    "\n",
    "f' par_type1Error: {par_type1Error}' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Percentage of type2 errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'par_type2Error: 0.03980099502487562'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here actaul = opens, predicted = fails\n",
    "par_type2Error = par_data[par_data['actual'] == 'opens'].query('predicted == \"fails\"').shape[0]/par_data.shape[0]\n",
    "\n",
    "f'par_type2Error: {par_type2Error}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Proportion of Erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['opens', 'fails'], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " par_data.predicted.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008234688625836336"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# treat everything as \"opens\" and then find the ones that are \"fails\"\n",
    "\n",
    "par_Error1 = par_data.query('actual == \"fails\"').shape[0]/par_data.shape[0]\n",
    "par_Error1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9917653113741637"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par_Error2 = par_data.query('actual == \"opens\"').shape[0]/par_data.shape[0]\n",
    "par_Error2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``NB:``\n",
    "\n",
    "> If every parachute was predicted to not open, the proportion of Type I Errors made.would be zero - becuase this prevents us from having type 1 errors by maximizin type2 errors\n",
    "\n",
    "> However, If every parachute was predicted to not open, the proportion of Type II Errors made would be ```0.9917653113741637```\n",
    "\n",
    "> This is also vice-versa, depenfing on which error in the question is ``maximiased and which one is minimized`` -- \n",
    "\n",
    "   > Also if very parachute was predicted as opens, the proportion of type1 errors that would be mased would be ```0.008234688625836336```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
